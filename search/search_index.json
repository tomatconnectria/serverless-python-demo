{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":""},{"location":"#the-pragmatic-python-serverless-developer","title":"The Pragmatic Python Serverless Developer","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Starting a Serverless service can be overwhelming. You need to figure out many questions and challenges that have nothing to do with your business domain:</p> <ul> <li>How to deploy to the cloud? What IAC framework do you choose?</li> <li>How to write a SaaS-oriented CI/CD pipeline? What does it need to contain?</li> <li>How do you write a Lambda function?</li> <li>How do you handle observability, logging, tracing, metrics?</li> <li>How do you handle testing?</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>This project provides a serverless service which is based an opinionated approach to Python project setup, testing, profiling, deployments, and operations. Learn about many open source tools, including Powertools for AWS Lambda\u2014a toolkit that can help you implement serverless best practices and increase developer velocity.</p>"},{"location":"#serverless-template","title":"Serverless Template","text":"<p>The service we present here started from the AWS Lambda Handler Cookbook, a serverless service template project.</p> <p>The template project provides a working, deployable, open source-based, serverless service template with an AWS Lambda function and AWS CDK Python code with all the best practices and a complete CI/CD pipeline.</p> <p>You can start your own service in three clicks.</p>"},{"location":"#the-products-service","title":"The Products service","text":"<p>Features\u00b6 <ul> <li>Python Serverless service with a recommended file structure.</li> <li>CDK infrastructure with infrastructure tests and security tests.</li> <li>Both synchronous and asynchronous resources</li> <li>CI/CD pipelines based on Github actions that deploys to AWS with python linters, complexity checks and style formatters.</li> <li>CI/CD pipeline deploys to dev/staging and production environment with different gates between each environment</li> <li>Makefile for simple developer experience.</li> <li>The AWS Lambda handler embodies Serverless best practices and has all the bells and whistles for a proper production ready handler.</li> <li>AWS Lambda handler uses AWS Lambda Powertools: logger, tracer, metrics, event handler, validation, batch</li> <li>AWS Lambda handler three layer architecture: handler layer, logic layer and data access layer (integration)</li> <li>Cognito user pool with a test user. All E2E tests automatically login with the test user and use a JWT id token</li> <li>Idempotent API</li> <li>Authentication protected API</li> <li>REST API protected by WAF with four AWS managed rules in production deployment</li> <li>CloudWatch dashboards - High level and low level including CloudWatch alarms</li> <li>Unit, infrastructure, security, integration and end to end tests.</li> </ul> <p>While the code examples are written in Python, the principles are valid to any supported AWS Lambda handler programming language.</p>"},{"location":"#monitoring-design","title":"Monitoring Design","text":"<p>Async Testing Design\u00b6 <p>License\u00b6 <p>This library is licensed under the MIT License. See the LICENSE file.</p> <p>Maintained by Ran Isenberg and Heitor Lessa</p>"},{"location":"decision_log/","title":"Decision log","text":""},{"location":"decision_log/#decision-log","title":"Decision log","text":"<p>This file is meant to capture project-level decisions that were made in this project and why. There are often no obvious correct answers, and we must decide with multiple options.</p>"},{"location":"decision_log/#2023-10-30","title":"2023-10-30","text":"<p>Added Amazon Cognito user pool. While it is not connected as external identity provider, or provides registration, it is a good start for any service. We build a test user in the CDK, and set its password, upload it to secrets manager and use it in the E2E tests to trigger the protected API. Its a good start to have a protected API during development prior to productization.</p>"},{"location":"decision_log/#2023-10-17","title":"2023-10-17","text":"<p>Added serverless monitoring with CloudWatch dashboards: logs, metrics, custom metrics and alarms.</p> <p>We use 'cdk-monitoring-constructs' open-source because it simplifies the dashboard creation and it is very simple to use.</p> <p>Based on concepts and examples described here</p>"},{"location":"decision_log/#2023-08-17","title":"2023-08-17","text":""},{"location":"decision_log/#project-structure","title":"Project Structure","text":""},{"location":"decision_log/#what","title":"What","text":"<p>We chose an opinionated project structure with an infrastructure folder (CDK-based), a tests folder, and a service folder containing business domain Lambda function code with a makefile to automate developer actions.</p>"},{"location":"decision_log/#why","title":"Why","text":"<p>This structure has proven its worth in production for us. However, there's no right or wrong; other structures might make sense to you.</p> <p>You can read more about it here.</p>"},{"location":"decision_log/#cdk","title":"CDK","text":""},{"location":"decision_log/#what_1","title":"What","text":"<p>We chose AWS CDK as the IaC of choice.</p>"},{"location":"decision_log/#why_1","title":"Why","text":"<ul> <li>There are many IaC options. We chose CDK since we enjoy working with it and have a very positive experience with it and with the experience of defining resources in code instead of YAML files.</li> <li>Choose what fits your organization best: AWS SAM, Serverless, Terraform, Pulumi, etc.</li> </ul>"},{"location":"decision_log/#cdk-constructs-structure","title":"CDK Constructs Structure","text":""},{"location":"decision_log/#what_2","title":"What","text":"<p>We defined a stack that creates two constructs: one for the crud API and one for the stream processing. The CRUD API also makes the database construct.</p>"},{"location":"decision_log/#why_2","title":"Why","text":"<p>We chose a business domain-driven Constructs approach. Each domain gets its construct, with the DB being the exception as an \"inner\" construct.</p> <p>We don't think there's a right or wrong approach to picking resources into constructs as long as it makes sense to you and you can find help and configurations easily.</p> <p>However, choosing a business domain-driven approach to selecting which resources belong together in a construct makes sense the most.</p> <p>Finding resources and understanding their connections is more accessible by looking at the service architecture diagram. It's also easier to share design patterns across teams in organizations that require the same architecture.</p> <p>You can read more about it with a similar example here</p>"},{"location":"decision_log/#cdk-best-practices","title":"CDK Best Practices","text":""},{"location":"decision_log/#what_3","title":"What","text":"<ul> <li>Stack per developer per branch, stack name is different</li> <li>Shared resources are built on the stack and passed as parameters to the constructs init functions.</li> <li>Lambda roles define inline policy definitions instead of using CDK's built-in functions</li> </ul>"},{"location":"decision_log/#why_3","title":"Why","text":"<ul> <li>Stack per developer per branch - we wanted multiple developers to share a dev account and work in parallel on the same stack. The CI/CD main pipeline has its unique name to remove any chance of conflicts.</li> <li>Shared resources - made sense to build once and pass internal resources such as Lambda layers.</li> <li>Lambda roles define inline policy definitions instead of using CDK's built-in functions - CDK's built-in 'grant' function is less privileged and provides more resources than required. They also reduce visibility and abstract actual permissions too much.</li> </ul> <p>You can read more about it here</p>"},{"location":"decision_log/#lambda-layers-usage","title":"Lambda Layers Usage","text":""},{"location":"decision_log/#what_4","title":"What","text":"<p>We use a Lambda layer that all our Lambda functions use.</p>"},{"location":"decision_log/#why_4","title":"Why","text":"<p>We use it as a deployment optimization since all our functions require mostly (or the same) dependencies.</p> <p>It comprises all '[tool.poetry.dependencies]' in the 'pyproject. toml' file.</p> <p>You can read more about creating Lambda layers here and best practices here.</p>"},{"location":"decision_log/#build-folder","title":".build folder","text":""},{"location":"decision_log/#what_5","title":"What","text":"<p>We use as build stage as part of 'make deploy' to copy the Lambda contents from 'product' folder to '.build'.</p>"},{"location":"decision_log/#why_5","title":"Why","text":"<p>You must supply an asset folder when building a Lambda layer/lambda function with CDK. It removes the top folder and takes the contents.</p> <p>If we were to supply the 'product' folder as the root folder, we would get import issues when invoking the function since the imports in our lambda function contain 'product.x.y' same as it resides on GitHub.</p> <p>To solve this issue, we have a build step that it runs when you run 'make deploy'; it copies the 'product' folder from the root level to a new root level folder, the '.build.'</p> <p>This way, when CDK takes the lambda contents from this new top level, it also takes the 'product' top folder and the imports remain valid.</p>"},{"location":"decision_log/#lambda-architecture-layers","title":"Lambda architecture layers","text":""},{"location":"decision_log/#what_6","title":"What","text":"<p>Under product, you have several folders: one per domain.</p> <p>Each domain: crud and stream processor, have different layers.</p> <p>We have different architectural layers: handler -&gt; domain logic -&gt; data access layer.</p> <p>Each layer has folders for Pydantic schema classes and utilities.</p>"},{"location":"decision_log/#why_6","title":"Why","text":"<p>This is an opinionated structure backed by AWS best practices to separate handler code from domain logic.</p> <p>You can read more about it here.</p>"},{"location":"decision_log/#testing-methodology","title":"Testing methodology","text":""},{"location":"decision_log/#what_7","title":"What","text":"<p>We have unit tests, infrastructure tests, integration tests, and end-to-end tests.</p>"},{"location":"decision_log/#why_7","title":"Why","text":"<p>Each test type has its usage:</p> <ul> <li>Unit tests check small functions and mostly schema validations.</li> <li>Infrastructure tests are run before deployment; they check that critical resources exist and were not deleted from the CloudFormation template (CDK output) by mistake or bug.</li> <li>Integration tests occur after deployment and generate a mocked event, call the function handler in the IDE and allow debugging the functions with breakpoints. We call real AWs services and can choose what to mock to simulate failures and what resources to call directly.</li> <li>E2E tests - trigger the AWS deployed resources.</li> </ul> <p>You can read about testing methodology and how to test serverless in this blog series</p>"},{"location":"decision_log/#pydantic-usage","title":"Pydantic Usage","text":""},{"location":"decision_log/#what_8","title":"What","text":"<p>We are using pydantic for environment variables parsing, schema validation of input/output, and more.</p>"},{"location":"decision_log/#why_8","title":"Why","text":"<p>Pydantic is a class leader parsing and validation Python library.</p> <p>Input validation is a critical security aspect that each application must implement.</p> <p>Read more about input validation in Lambda here.</p> <p>Read more about why you should care about environment variables parsing here.</p>"},{"location":"monitoring/","title":"Monitoring","text":""},{"location":"monitoring/#key-concepts","title":"Key Concepts","text":"<p>Utilizing AWS CloudWatch dashboards enables centralized monitoring of API Gateway, Lambda functions, and DynamoDB, providing real-time insights into their performance and operational health.</p> <p>By aggregating metrics, logs, and alarms, CloudWatch facilitates swift issue diagnosis and analysis across your serverless applications. Additionally, setting up alarms ensures immediate alerts during anomalous activities, enabling proactive issue mitigation.</p>"},{"location":"monitoring/#service-architecture","title":"Service Architecture","text":"<p>The goal is to monitor the service API gateway, Lambda function, and DynamoDB tables and ensure everything is in order.</p> <p>In addition, we want to visualize service KPI metrics.</p>"},{"location":"monitoring/#monitoring-dashboards","title":"Monitoring Dashboards","text":"<p>We will define two dashboards:</p> <ul> <li>High level</li> <li>Low level</li> </ul> <p>Each dashboard has its usage and tailors different personas' usage.</p>"},{"location":"monitoring/#high-level-dashboard","title":"High Level Dashboard","text":"<p>We have a high level dashboard per module: CRUD and Streaming.</p> <p>This dashboard is designed to be an executive overview of the service.</p> <p>Total API gateway metrics provide information on the performance and error rate of the service.</p> <p>KPI metrics are included in the bottom part as well.</p> <p>Personas that use this dashboard: SRE, developers, and product teams (KPIs)</p> <p></p>"},{"location":"monitoring/#low-level-dashboard","title":"Low Level Dashboard","text":"<p>It is aimed at a deep dive into all the service's resources. Requires an understanding of the service architecture and its moving parts.</p> <p>The dashboard provides the Lambda function's metrics for latency, errors, throttles, provisioned concurrency, and total invocations.</p> <p>In addition, a CloudWatch logs widget shows only 'error' logs from the Lambda function.</p> <p>As for DynamoDB tables, we have the primary database and the idempotency table for usage, operation latency, errors, and throttles.</p> <p>Personas that use this dashboard: developers, SREs.</p> <p>We have two dashboards : one for CRUD module and the other for the streaming module.</p>"},{"location":"monitoring/#crud","title":"CRUD","text":""},{"location":"monitoring/#streaming","title":"Streaming","text":""},{"location":"monitoring/#alarms","title":"Alarms","text":"<p>Having visibility and information is one thing, but being proactive and knowing beforehand that a significant error is looming is another. A CloudWatch</p> <p>Alarm is an automated notification tool within AWS CloudWatch that triggers alerts based on user-defined thresholds, enabling users to identify and</p> <p>respond to operational issues, breaches, or anomalies in AWS resources by monitoring specified metrics over a designated period.</p> <p>In this dashboard, you will find an example of two types of alarms:</p> <ul> <li>Alarm for performance threshold monitoring</li> <li>Alarm for error rate threshold monitoring</li> </ul> <p>For latency-related issues, we define the following alarm:</p> <p></p> <p>For P90, P50 metrics, follow this explanation.</p> <p>For internal server errors rate, we define the following alarm: </p>"},{"location":"monitoring/#actions","title":"Actions","text":"<p>Alarms are of no use unless they have an action. We have configured the alarms to send an SNS notification to a new SNS topic. From there, you can connect any subscription - HTTPS/SMS/Email, etc. to notify your teams with the alarm details.</p>"},{"location":"monitoring/#cdk-reference","title":"CDK Reference","text":"<p>We use the open-source cdk-monitoring-constructs.</p> <p>You can view find the monitoring CDK construct here.</p>"},{"location":"monitoring/#further-reading","title":"Further Reading","text":"<p>If you wish to learn more about this concept and go over details on the CDK code, check out my blog post.</p>"},{"location":"opensource/","title":"Open Source","text":""},{"location":"opensource/#open-source","title":"Open Source","text":"<p>This page lists all open source libraries this project currently uses.</p> <ul> <li>aws-lambda-powertools</li> <li>serverless template</li> <li>mypy</li> <li>pydantic</li> <li>cachetools</li> <li>aws-lambda-env-modeler</li> <li>cdk-nag</li> <li>cdk-monitoring-constructs</li> <li>radon</li> <li>xenon</li> <li>pre-commit</li> <li>ruff</li> <li>mkdocs-material</li> <li>AWS CDK</li> </ul>"},{"location":"pipeline/","title":"CI/CD Pipeline","text":""},{"location":"pipeline/#getting-started","title":"Getting Started","text":"<p>The GitHub CI/CD pipeline includes the following steps.</p> <p>The pipelines uses environment secrets (under the defined environment 'dev', 'staging' and 'production') for code coverage and for the role to deploy to AWS.</p> <p>When you clone this repository be sure to define the environments in your repo settings and create a secret per environment:</p> <ul> <li>AWS_ROLE - to role to assume for your GitHub worker as defined here</li> </ul>"},{"location":"pipeline/#makefile-commands","title":"Makefile Commands","text":"<p>All steps can be run locally using the makefile. See details below:</p> <ul> <li>Create Python environment ands install dev dependencie with <code>make dev</code></li> <li>Run pre-commit checks as defined in <code>.pre-commit-config.yaml</code></li> <li>Lint and format and sort imports with ruff (similar to flake8/yapf/isort) - run <code>make format</code> in the IDE</li> <li>Static type check with mypy - run <code>make lint</code> in the IDE</li> <li>Verify that Python imports are sorted according to standard - run <code>make sort</code> in the IDE</li> <li>Python complexity checks: radon and xenon  - run <code>make complex</code> in the IDE</li> <li>Unit tests. Run <code>make unit</code> to run unit tests in the IDE</li> <li>Infrastructure test. Run <code>make infra-tests</code> to run the CDK infrastructure tests in the IDE</li> <li>Code coverage by codecov.io</li> <li>Deploy CDK - run <code>make deploy</code> in the IDE, will also run security tests based on cdk_nag</li> <li>E2E tests  - run <code>make e2e</code> in the IDE</li> <li>Code coverage tests  - run <code>make coverage-tests</code> in the IDE after CDK dep</li> </ul>"},{"location":"pipeline/#other-capabilities","title":"Other Capabilities","text":"<ul> <li>Automatic Python dependencies update with Dependabot</li> <li>Easy to use makefile allows to run locally all commands in the GitHub actions</li> <li>Run local docs server, prior to push in pipeline - run <code>make docs</code>  in the IDE</li> <li>Prepare PR, run all checks with one command - run <code>make pr</code> in the IDE</li> </ul>"},{"location":"pipeline/#environments-pipelines","title":"Environments &amp; Pipelines","text":"<p>All GitHub workflows are stored under <code>.github/workflows</code> folder.</p> <p>The two most important ones are <code>pr-serverless-service</code>  and <code>main-serverless-service</code>.</p>"},{"location":"pipeline/#pr-serverless-service","title":"pr-serverless-service","text":"<p><code>pr-serverless-service</code> runs for every pull request you open. It expects you defined a GitHub environment by the name <code>dev</code> and that it includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes two jobs: 'quality_standards' and 'tests' where a failure in 'quality_standards' does not trigger 'tests'. Both jobs MUST pass in order to to be able to merge.</p> <p>'quality_standards' includes all linters, pre-commit checks and units tests and 'tests' deploys the service to AWS, runs code coverage checks, security checks and E2E tests. Stack is destroyed at the end. Stack has a 'dev' prefix as part of its name.</p> <p>Once merged, <code>main-serverless-service</code> will run.</p>"},{"location":"pipeline/#main-serverless-service","title":"main-serverless-service","text":"<p><code>main-serverless-service</code> runs for every MERGED pull request that runs on the main branch. It expects you defined a GitHub environments by the name <code>staging</code> and <code>production</code> and that both includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes three jobs: 'staging', 'production' and 'publish_github_pages'.</p> <p>'staging' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It runs just coverage tests and E2E tests. Stack is not deleted. Stack has a 'staging' prefix as part of its name. Any failure in staging will stop the pipeline and production environment will not get updated with the new code.</p> <p>'production' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It does not run any test at the moment. Stack is not deleted. Stack has a 'production' prefix as part of its name.</p>"},{"location":"api/product_models/","title":"Product models","text":""},{"location":"api/product_models/#product-models","title":"Product models","text":""},{"location":"api/product_models/#product.models.products.product-attributes","title":"Attributes","text":""},{"location":"api/product_models/#product.models.products.product.ProductId","title":"<code>ProductId = Annotated[str, Field(min_length=36, max_length=36), AfterValidator(validate_product_id)]</code>  <code>module-attribute</code>","text":"<p>Unique Product ID, represented and validated as a UUID string.</p>"},{"location":"api/product_models/#product.models.products.product-classes","title":"Classes","text":""},{"location":"api/product_models/#product.models.products.product.ProductEntry","title":"<code>ProductEntry</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Data representation for a product in a DynamoDB. In advanced services, ProductEntry is different than Product</p> PARAMETER  DESCRIPTION <code>name</code> <p>Product name</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Product ID (UUID string)</p> <p> TYPE: <code>ProductId</code> </p> <code>price</code> <p>Product price represented as a positive integer</p> <p> TYPE: <code>PositiveInt</code> </p>"},{"location":"api/product_models/#product.models.products.product.ProductEntry-attributes","title":"Attributes","text":"<code>name: Annotated[str, Field(min_length=1, max_length=50)]</code> <code>instance-attribute</code> \u00b6 <code>id: ProductId</code> <code>instance-attribute</code> \u00b6 <code>price: PositiveInt</code> <code>instance-attribute</code> \u00b6 <code>created_at: PositiveInt</code> <code>instance-attribute</code> \u00b6"},{"location":"api/product_models/#product.models.products.product-functions","title":"Functions","text":""},{"location":"api/product_models/#validators","title":"Validators","text":""},{"location":"api/product_models/#product.models.products.validators-functions","title":"Functions","text":""},{"location":"api/product_models/#product.models.products.validators.validate_product_id","title":"<code>validate_product_id(product_id)</code>","text":"<p>Validates Product IDs are valid UUIDs</p> PARAMETER  DESCRIPTION <code>product_id</code> <p>Product ID as a string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Validated product ID value</p> RAISES DESCRIPTION <code>ValueError</code> <p>When a product ID doesn't conform with the UUID spec.</p> Source code in <code>product/models/products/validators.py</code> <pre><code>def validate_product_id(product_id: str) -&gt; str:\n    \"\"\"Validates Product IDs are valid UUIDs\n\n    Parameters\n    ----------\n    product_id : str\n        Product ID as a string\n\n    Returns\n    -------\n    str\n        Validated product ID value\n\n    Raises\n    ------\n    ValueError\n        When a product ID doesn't conform with the UUID spec.\n    \"\"\"\n    try:\n        UUID(product_id, version=4)\n    except Exception as exc:  # pragma: no cover\n        raise ValueError(str(exc)) from exc\n    return product_id\n</code></pre>"},{"location":"api/stream_processor/","title":"Stream Processor","text":""},{"location":"api/stream_processor/#product-notification","title":"Product notification","text":""},{"location":"api/stream_processor/#lambda-handlers","title":"Lambda Handlers","text":"<p>Process stream is connected to Amazon DynamoDB Stream that polls product changes in the product table.</p> <p>We convert them into <code>ProductChangeNotification</code> model depending on the DynamoDB Stream Event Name (e.g., <code>INSERT</code> -&gt; <code>ADDED</code>).</p>"},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream-functions","title":"Functions","text":""},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream.process_stream","title":"<code>process_stream(event, context, event_handler=None)</code>","text":"<p>Process batch of Amazon DynamoDB Stream containing product changes.</p> PARAMETER  DESCRIPTION <code>event</code> <p>DynamoDB Stream event.</p> <p>See sample</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>context</code> <p>Lambda Context object.</p> <p>It is used to enrich our structured logging via Powertools for AWS Lambda.</p> <p>See sample</p> <p> TYPE: <code>LambdaContext</code> </p> <code>event_handler</code> <p>Event Handler to use to notify product changes, by default <code>EventHandler</code> with EventBridge as a provider</p> <p> TYPE: <code>BaseEventHandler | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream.process_stream--integrations","title":"Integrations","text":""},{"location":"api/stream_processor/#product.stream_processor.handlers.process_stream.process_stream--domain","title":"Domain","text":"<ul> <li><code>notify_product_updates</code> to notify <code>ProductChangeNotification</code> changes</li> </ul> RETURNS DESCRIPTION <code>Dict</code> <p>Receipts for unsuccessfully and successfully published events.</p> RAISES DESCRIPTION <code>ProductChangeNotificationDeliveryError</code> <p>Partial or total failures when sending notification. It allows the stream to stop at the exact same sequence number.</p> <p>This means sending notifications are at least once.</p> Source code in <code>product/stream_processor/handlers/process_stream.py</code> <pre><code>@init_environment_variables(model=PrcStreamVars)\n@logger.inject_lambda_context(log_event=True)\n@metrics.log_metrics\n@tracer.capture_lambda_handler(capture_response=False)\ndef process_stream(\n    event: dict[str, Any],\n    context: LambdaContext,\n    event_handler: BaseEventHandler | None = None,\n) -&gt; dict:\n    \"\"\"Process batch of Amazon DynamoDB Stream containing product changes.\n\n\n    Parameters\n    ----------\n    event : dict[str, Any]\n        DynamoDB Stream event.\n\n        See [sample](https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html#events-sample-dynamodb)\n    context : LambdaContext\n        Lambda Context object.\n\n        It is used to enrich our structured logging via Powertools for AWS Lambda.\n\n        See [sample](https://docs.aws.amazon.com/lambda/latest/dg/python-context.html)\n    event_handler : BaseEventHandler | None, optional\n        Event Handler to use to notify product changes, by default `EventHandler` with EventBridge as a provider\n\n    Integrations\n    ------------\n\n    # Domain\n\n    * `notify_product_updates` to notify `ProductChangeNotification` changes\n\n    Returns\n    -------\n    Dict\n        Receipts for unsuccessfully and successfully published events.\n\n    Raises\n    ------\n    ProductChangeNotificationDeliveryError\n        Partial or total failures when sending notification. It allows the stream to stop at the exact same sequence number.\n\n        This means sending notifications are at least once.\n    \"\"\"\n    # Until we create our handler product stream change input\n    stream_records = DynamoDBStreamEvent(event)\n\n    env_vars = get_environment_variables(model=PrcStreamVars)\n    logger.debug('environment variables', env_vars=env_vars.model_dump())\n\n    metrics.add_metric(name='StreamRecords', unit=MetricUnit.Count, value=len(stream_records.keys()))\n\n    product_updates = []\n    for record in stream_records.records:\n        product_id = record.dynamodb.keys.get('id', '')  # type: ignore[union-attr]\n        logger.append_keys(product_id=product_id)\n        logger.info('handling record', event_name=record.event_name)\n\n        match record.event_name:\n            case record.event_name.INSERT:  # type: ignore[union-attr]\n                product_updates.append(ProductChangeNotification(product_id=product_id, status='ADDED'))\n            case record.event_name.REMOVE:  # type: ignore[union-attr]\n                product_updates.append(ProductChangeNotification(product_id=product_id, status='REMOVED'))\n\n    if event_handler is None:  # pragma: no cover\n        event_handler = EventHandler(event_source=env_vars.EVENT_SOURCE, event_bus=env_vars.EVENT_BUS)\n\n    receipt = notify_product_updates(update=product_updates, event_handler=event_handler)\n\n    return receipt.model_dump()\n</code></pre>"},{"location":"api/stream_processor/#domain-logic","title":"Domain logic","text":"<p>Domain logic to notify product changes, e.g., <code>ADDED</code>, <code>REMOVED</code>, <code>UPDATED</code>.</p>"},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification-functions","title":"Functions","text":""},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification.notify_product_updates","title":"<code>notify_product_updates(update, event_handler)</code>","text":"<p>Notify product change notifications using default or provided event handler.</p> PARAMETER  DESCRIPTION <code>update</code> <p>List of product change notifications to notify.</p> <p> TYPE: <code>list[ProductChangeNotification]</code> </p> <code>event_handler</code> <p>Event handler to use for notification</p> <p> TYPE: <code>BaseEventHandler</code> </p>"},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification.notify_product_updates--environment-variables","title":"Environment variables","text":"<p><code>EVENT_BUS</code> : Event Bus to notify product change notifications</p>"},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification.notify_product_updates--examples","title":"Examples","text":"<p>Sending a newly added product notification</p> <pre><code>from product.stream_processor.domain_logic.product_notification import notify_product_updates\n\n\nnotification = ProductChangeNotification(product_id=product_id, status=\"ADDED\")\nevent_handler =\nreceipt = notify_product_updates(update=[notification])\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification.notify_product_updates--integrations","title":"Integrations","text":""},{"location":"api/stream_processor/#product.stream_processor.domain_logic.product_notification.notify_product_updates--events","title":"Events","text":"<ul> <li><code>EventHandler</code> uses <code>EventBridge</code> provider to convert and publish <code>ProductChangeNotification</code> models into events.</li> <li><code>EventHandler</code> uses <code>EventBridge</code> provider to convert and publish <code>ProductChangeNotification</code> models into events.</li> </ul> RETURNS DESCRIPTION <code>EventReceipt</code> <p>Receipts for unsuccessfully and successfully published events.</p> Source code in <code>product/stream_processor/domain_logic/product_notification.py</code> <pre><code>def notify_product_updates(update: list[ProductChangeNotification], event_handler: BaseEventHandler) -&gt; EventReceipt:\n    \"\"\"Notify product change notifications using default or provided event handler.\n\n    Parameters\n    ----------\n    update : list[ProductChangeNotification]\n        List of product change notifications to notify.\n    event_handler : BaseEventHandler\n        Event handler to use for notification\n\n    Environment variables\n    ---------------------\n    `EVENT_BUS` : Event Bus to notify product change notifications\n\n    # Examples\n\n    Sending a newly added product notification\n\n    ```python\n    from product.stream_processor.domain_logic.product_notification import notify_product_updates\n\n\n    notification = ProductChangeNotification(product_id=product_id, status=\"ADDED\")\n    event_handler =\n    receipt = notify_product_updates(update=[notification])\n    ```\n\n    Integrations\n    ------------\n\n    # Events\n\n    * `EventHandler` uses `EventBridge` provider to convert and publish `ProductChangeNotification` models into events.\n    * `EventHandler` uses `EventBridge` provider to convert and publish `ProductChangeNotification` models into events.\n\n    Returns\n    -------\n    EventReceipt\n        Receipts for unsuccessfully and successfully published events.\n    \"\"\"\n    return event_handler.emit(payload=update)\n</code></pre>"},{"location":"api/stream_processor/#integrations","title":"Integrations","text":"<p>These are integrations with external services. As of now, we only use one integration to send events, by default <code>Amazon EventBridge</code>.</p> <p>NOTE: We could make a single Event Handler. For now, we're using one event handler closely aligned with the model we want to convert into event for type safety.</p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler.EventHandler","title":"<code>EventHandler(event_source, event_bus, provider=None)</code>","text":"<p>             Bases: <code>BaseEventHandler[AnyModel]</code></p> <p>Event Handler for emitting events with a given provider.</p> PARAMETER  DESCRIPTION <code>event_source</code> <p>Event source to inject in event metadata, following 'myorg.service_name.feature_name'</p> <p> TYPE: <code>str</code> </p> <code>event_bus</code> <p>Event bus to send events to</p> <p> TYPE: <code>str</code> </p> <code>provider</code> <p>An event provider to send events to, by default EventBridge if omitted.</p> <p> TYPE: <code>BaseEventProvider</code> DEFAULT: <code>None</code> </p> Source code in <code>product/stream_processor/integrations/events/event_handler.py</code> <pre><code>def __init__(self, event_source: str, event_bus: str, provider: BaseEventProvider | None = None) -&gt; None:\n    \"\"\"Event Handler for emitting events with a given provider.\n\n    Parameters\n    ----------\n    event_source : str\n        Event source to inject in event metadata, following 'myorg.service_name.feature_name'\n    event_bus: str\n        Event bus to send events to\n    provider : BaseEventProvider\n        An event provider to send events to, by default EventBridge if omitted.\n    \"\"\"\n    self.provider = provider or EventBridge(bus_name=event_bus)\n    super().__init__(event_source=event_source, event_bus=event_bus, provider=self.provider)\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler.EventHandler-attributes","title":"Attributes","text":"<code>provider = provider or EventBridge(bus_name=event_bus)</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler.EventHandler-functions","title":"Functions","text":"<code>emit(payload, metadata=None, correlation_id='')</code> \u00b6 <p>Converts and emits a list of models into standard events with extra metadata and correlation ID.</p> PARAMETER  DESCRIPTION <code>payload</code> <p> TYPE: <code>list[AnyModel]</code> </p> <code>payload</code> <p>List of product change notifications models to be sent.</p> <p> TYPE: <code>list[AnyModel]</code> </p> <code>metadata</code> <p>Additional metadata to be injected into the event before sending, by default None</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>correlation_id</code> <p>Correlation ID to inject in event metadata. We generate one if not provided.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>EventReceipt</code> <p>Receipts for unsuccessfully and successfully published events.</p> Source code in <code>product/stream_processor/integrations/events/event_handler.py</code> <pre><code>def emit(self, payload: list[AnyModel], metadata: dict[str, Any] | None = None, correlation_id='') -&gt; EventReceipt:\n    \"\"\"Converts and emits a list of models into standard events with extra metadata and correlation ID.\n\n    Parameters\n    ----------\n    payload : list[AnyModel]\n    payload : list[AnyModel]\n        List of product change notifications models to be sent.\n    metadata : dict[str, Any] | None, optional\n        Additional metadata to be injected into the event before sending, by default None\n    correlation_id : str, optional\n        Correlation ID to inject in event metadata. We generate one if not provided.\n\n    Returns\n    -------\n    EventReceipt\n        Receipts for unsuccessfully and successfully published events.\n    \"\"\"\n    event_payload = EventHandler.build_events_from_models(\n        models=payload,\n        metadata=metadata,\n        correlation_id=correlation_id,\n        event_source=self.event_source,\n    )\n    return self.provider.send(payload=event_payload)\n</code></pre> <code>extract_event_name_from_model(model)</code> <code>staticmethod</code> \u00b6 <p>Derives a standard event name from the name of the model.</p> <p>It uses snake_case in uppercase letters, e.g., <code>ProductNotification</code> -&gt; <code>PRODUCT_NOTIFICATION</code>.</p> <p>It also keeps numbers and acronyms that are typically abbreviation for something intact, e.g.: \"ProductHTTP\" -&gt; \"PRODUCT_HTTP\"</p> PARAMETER  DESCRIPTION <code>model</code> <p>BaseModel to derive name from.</p> <p> TYPE: <code>AnyModel</code> </p> <code>build_events_from_models(models, event_source, metadata=None, correlation_id='')</code> <code>staticmethod</code> \u00b6 <p>Converts a Pydantic model into a standard event.</p> PARAMETER  DESCRIPTION <code>models</code> <p>List of Pydantic models to convert into events.</p> <p> TYPE: <code>list[AnyModel]</code> </p> <code>event_source</code> <p>Event source name to inject into event metadata.</p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>Additional metadata to inject, by default None</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>correlation_id</code> <p>Correlation ID to use in event metadata. If not provided, we generate one using UUID4.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler.EventHandler.extract_event_name_from_model--examples","title":"Examples","text":"<p>Building event name from a given model</p> <pre><code>from pydantic import BaseModel\nfrom product.stream_processor.integrations.events.event_handler import EventHandler\n\nclass SampleNotification(BaseModel):\n    message: str\n\nnotification = SampleNotification(message='testing')\nevent_name = EventHandler.convert_model_to_event_name(notification)\n\nassert event_name == \"SAMPLE_NOTIFICATION\"\n</code></pre> RETURNS DESCRIPTION <code>str</code> <p>Standard event name in snake_case upper letters.</p> Source code in <code>product/stream_processor/integrations/events/event_handler.py</code> <pre><code>@staticmethod\ndef extract_event_name_from_model(model: AnyModel) -&gt; str:\n    \"\"\"Derives a standard event name from the name of the model.\n\n    It uses snake_case in uppercase letters, e.g., `ProductNotification` -&gt; `PRODUCT_NOTIFICATION`.\n\n    It also keeps numbers and acronyms that are typically abbreviation for something intact, e.g.: \"ProductHTTP\" -&gt; \"PRODUCT_HTTP\"\n\n    Parameters\n    ----------\n    model : AnyModel\n        BaseModel to derive name from.\n\n    # Examples\n\n    Building event name from a given model\n\n    ```python\n    from pydantic import BaseModel\n    from product.stream_processor.integrations.events.event_handler import EventHandler\n\n    class SampleNotification(BaseModel):\n        message: str\n\n    notification = SampleNotification(message='testing')\n    event_name = EventHandler.convert_model_to_event_name(notification)\n\n    assert event_name == \"SAMPLE_NOTIFICATION\"\n    ```\n\n    Returns\n    -------\n    str\n        Standard event name in snake_case upper letters.\n    \"\"\"\n    model_name: str = model.__class__.__name__\n    return _pascal_to_snake_pattern.sub(r'_\\1', model_name).upper()\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.event_handler.EventHandler.build_events_from_models--examples","title":"Examples","text":"<p>Building standard events from a list of models</p> <pre><code>from pydantic import BaseModel\nfrom product.stream_processor.integrations.events.event_handler import EventHandler\n\nclass SampleNotification(BaseModel):\n    message: str\n\nnotification = SampleNotification(message='testing')\nevent_source = \"myorg.product.product_change_stream\"\nevents = EventHandler.build_events_from_models(models=[notification], event_source=event_source)\n\nevent = events[0]\n\nassert event.event_source == event_source\nassert event.data == notification\n</code></pre> RETURNS DESCRIPTION <code>list[Event]</code> <p>List of events created from model ready to be emitted.</p> Source code in <code>product/stream_processor/integrations/events/event_handler.py</code> <pre><code>@staticmethod\ndef build_events_from_models(\n    models: list[AnyModel], event_source: str, metadata: dict[str, Any] | None = None, correlation_id: str = ''\n) -&gt; list[Event]:\n    \"\"\"Converts a Pydantic model into a standard event.\n\n    Parameters\n    ----------\n    models : list[AnyModel]\n        List of Pydantic models to convert into events.\n    event_source : str\n        Event source name to inject into event metadata.\n    metadata : dict[str, Any] | None, optional\n        Additional metadata to inject, by default None\n    correlation_id : str, optional\n        Correlation ID to use in event metadata. If not provided, we generate one using UUID4.\n\n\n    # Examples\n\n    Building standard events from a list of models\n\n    ```python\n    from pydantic import BaseModel\n    from product.stream_processor.integrations.events.event_handler import EventHandler\n\n    class SampleNotification(BaseModel):\n        message: str\n\n    notification = SampleNotification(message='testing')\n    event_source = \"myorg.product.product_change_stream\"\n    events = EventHandler.build_events_from_models(models=[notification], event_source=event_source)\n\n    event = events[0]\n\n    assert event.event_source == event_source\n    assert event.data == notification\n    ```\n\n    Returns\n    -------\n    list[Event]\n        List of events created from model ready to be emitted.\n    \"\"\"\n    metadata = metadata or {}\n    correlation_id = correlation_id or f'{uuid4()}'\n\n    events: list[Event] = []\n\n    for model in models:\n        event_name = EventHandler.extract_event_name_from_model(model=model)\n        event_version = getattr(model, '__version__', DEFAULT_EVENT_VERSION)  # defaults to v1\n\n        events.append(\n            Event(\n                data=model,\n                metadata=EventMetadata(\n                    event_name=event_name, event_source=event_source, event_version=event_version, correlation_id=correlation_id, **metadata\n                ),\n            )\n        )\n\n    return events\n</code></pre>"},{"location":"api/stream_processor/#events","title":"Events","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input.AnyModel","title":"<code>AnyModel = TypeVar('AnyModel', bound=BaseModel)</code>  <code>module-attribute</code>","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input.EventMetadata","title":"<code>EventMetadata</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Data representation for a standard event metadata</p> PARAMETER  DESCRIPTION <code>event_name</code> <p>Name of the event, e.g., \"PRODUCT_CHANGE_NOTIFICATION\"</p> <p> TYPE: <code>str</code> </p> <code>event_source</code> <p>Event source, e.g., \"myorg.service.feature\"</p> <p> TYPE: <code>str</code> </p> <code>event_version</code> <p>Event version, e.g. \"v1\"</p> <p> TYPE: <code>str</code> </p> <code>correlation_id</code> <p>Correlation ID, e.g., \"b76d27e1-bd2b-4aae-9781-1ef11063c5cd\"</p> <p> TYPE: <code>str</code> </p> <p>created_at : datetime     Timestamp of when the event was created (UTC)</p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input.EventMetadata-attributes","title":"Attributes","text":"<code>event_name: str</code> <code>instance-attribute</code> \u00b6 <code>event_source: str</code> <code>instance-attribute</code> \u00b6 <code>event_version: str</code> <code>instance-attribute</code> \u00b6 <code>correlation_id: str</code> <code>instance-attribute</code> \u00b6 <code>created_at: datetime = Field(default_factory=datetime.utcnow)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(extra='allow')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input.Event","title":"<code>Event</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[AnyModel]</code></p> <p>Data representation for a standard event</p> PARAMETER  DESCRIPTION <code>data</code> <p>Any Pydantic BaseModel</p> <p> TYPE: <code>BaseModel</code> </p> <code>metadata</code> <p>Event metadata</p> <p> TYPE: <code>EventMetadata</code> </p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.input.Event-attributes","title":"Attributes","text":"<code>data: AnyModel</code> <code>instance-attribute</code> \u00b6 <code>metadata: EventMetadata</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceiptSuccess","title":"<code>EventReceiptSuccess</code>","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceiptSuccess-attributes","title":"Attributes","text":"<code>receipt_id: str</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceiptFail","title":"<code>EventReceiptFail</code>","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceiptFail-attributes","title":"Attributes","text":"<code>receipt_id: str</code> <code>instance-attribute</code> \u00b6 <code>error: str</code> <code>instance-attribute</code> \u00b6 <code>details: dict</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceipt","title":"<code>EventReceipt</code>","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.models.output.EventReceipt-attributes","title":"Attributes","text":"<code>success: list[EventReceiptSuccess]</code> <code>instance-attribute</code> \u00b6 <code>failed: list[EventReceiptFail] = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#providers","title":"Providers","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge.EventBridge","title":"<code>EventBridge(bus_name, client=None)</code>","text":"<p>             Bases: <code>BaseEventProvider</code></p> <p>Amazon EventBridge provider using PutEvents API.</p> <p>See PutEvents docs.</p> PARAMETER  DESCRIPTION <code>bus_name</code> <p>Name of the event bus to send events to</p> <p> TYPE: <code>str</code> </p> <code>client</code> <p>EventBridge boto3 client to use, by default None</p> <p> TYPE: <code>Optional[EventBridgeClient]</code> DEFAULT: <code>None</code> </p> Source code in <code>product/stream_processor/integrations/events/providers/eventbridge.py</code> <pre><code>def __init__(self, bus_name: str, client: Optional['EventBridgeClient'] = None):\n    \"\"\"Amazon EventBridge provider using PutEvents API.\n\n    See [PutEvents docs](https://docs.aws.amazon.com/eventbridge/latest/APIReference/API_PutEvents.html).\n\n    Parameters\n    ----------\n    bus_name : str\n        Name of the event bus to send events to\n    client : Optional[EventBridgeClient], optional\n        EventBridge boto3 client to use, by default None\n    \"\"\"\n    self.bus_name = bus_name\n    self.client = client or boto3.client('events')\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge.EventBridge-attributes","title":"Attributes","text":"<code>bus_name = bus_name</code> <code>instance-attribute</code> \u00b6 <code>client = client or boto3.client('events')</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge.EventBridge-functions","title":"Functions","text":"<code>send(payload)</code> \u00b6 <p>Sends batches of events up to maximum allowed by PutEvents API (10).</p> PARAMETER  DESCRIPTION <code>payload</code> <p>List of events to publish</p> <p> TYPE: <code>list[Event]</code> </p> RETURNS DESCRIPTION <code>EventReceipt</code> <p>Receipts for unsuccessfully and successfully published events</p> RAISES DESCRIPTION <code>ProductChangeNotificationDeliveryError</code> <p>When one or more events could not be delivered.</p> Source code in <code>product/stream_processor/integrations/events/providers/eventbridge.py</code> <pre><code>def send(self, payload: list[Event]) -&gt; EventReceipt:\n    \"\"\"Sends batches of events up to maximum allowed by PutEvents API (10).\n\n    Parameters\n    ----------\n    payload : list[Event]\n        List of events to publish\n\n    Returns\n    -------\n    EventReceipt\n        Receipts for unsuccessfully and successfully published events\n\n    Raises\n    ------\n    ProductChangeNotificationDeliveryError\n        When one or more events could not be delivered.\n    \"\"\"\n    success: list[EventReceiptSuccess] = []\n    failed: list[EventReceiptFail] = []\n    events = self.build_put_events_requests(payload)\n\n    for batch in events:\n        try:\n            result = self.client.put_events(Entries=batch)\n            ok, not_ok = self._collect_receipts(result)\n            success.extend(ok)\n            failed.extend(not_ok)\n        except botocore.exceptions.ClientError as exc:\n            error_message = exc.response['Error']['Message']\n\n            receipt = EventReceiptFail(receipt_id='', error='error_message', details=exc.response['ResponseMetadata'])\n            raise ProductChangeNotificationDeliveryError(f'Failed to deliver all events: {error_message}', receipts=[receipt]) from exc\n\n    return EventReceipt(success=success, failed=failed)\n</code></pre> <code>build_put_events_requests(payload)</code> \u00b6 <p>Converts a list of events into a list of PutEvents API request.</p> <p>If AWS X-Ray is enabled, it automatically includes 'TraceHeader' field in the request.</p> YIELDS DESCRIPTION <code>list[PutEventsRequestEntryTypeDef]</code> <p>List of maximum events permitted to be sent by a single PutEvents API.</p> Source code in <code>product/stream_processor/integrations/events/providers/eventbridge.py</code> <pre><code>def build_put_events_requests(self, payload: list[Event]) -&gt; Generator[list['PutEventsRequestEntryTypeDef'], None, None]:\n    \"\"\"Converts a list of events into a list of PutEvents API request.\n\n    If AWS X-Ray is enabled, it automatically includes 'TraceHeader' field in the request.\n\n    Yields\n    ------\n    list['PutEventsRequestEntryTypeDef']\n        List of maximum events permitted to be sent by a single PutEvents API.\n    \"\"\"\n    trace_id = os.environ.get(XRAY_TRACE_ID_ENV)\n\n    for chunk in chunk_from_list(events=payload, max_items=EVENTBRIDGE_PROVIDER_MAX_EVENTS_ENTRY):\n        events: list['PutEventsRequestEntryTypeDef'] = []\n\n        for event in chunk:\n            # 'Time' field is not included to be able to measure end-to-end latency later (time - created_at)\n            event_request: 'PutEventsRequestEntryTypeDef' = {\n                'Source': event.metadata.event_source,\n                'DetailType': event.metadata.event_name,\n                'Detail': event.model_dump_json(),\n                'EventBusName': self.bus_name,\n            }\n\n            if trace_id:\n                event_request['TraceHeader'] = trace_id\n\n            events.append(event_request)\n\n        yield events\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.providers.eventbridge-functions","title":"Functions","text":""},{"location":"api/stream_processor/#interfaces","title":"Interfaces","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.BaseEventProvider","title":"<code>BaseEventProvider</code>","text":"<p>             Bases: <code>ABC</code></p> <p>ABC for an Event Provider that send events to a destination.</p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.BaseEventProvider-functions","title":"Functions","text":"<code>send(payload)</code> <code>abstractmethod</code> \u00b6 <p>Sends list of events to an Event Provider.</p> PARAMETER  DESCRIPTION <code>payload</code> <p>List of events to send.</p> <p> TYPE: <code>list[Event]</code> </p> RETURNS DESCRIPTION <code>EventReceipt</code> <p>Receipts for unsuccessfully and successfully published events.</p> RAISES DESCRIPTION <code>NotificationDeliveryError</code> <p>When one or more events could not be delivered.</p> Source code in <code>product/stream_processor/integrations/events/base.py</code> <pre><code>@abstractmethod\ndef send(self, payload: list[Event]) -&gt; EventReceipt:\n    \"\"\"Sends list of events to an Event Provider.\n\n    Parameters\n    ----------\n    payload : list[Event]\n        List of events to send.\n\n    Returns\n    -------\n    EventReceipt\n        Receipts for unsuccessfully and successfully published events.\n\n    Raises\n    ------\n    NotificationDeliveryError\n        When one or more events could not be delivered.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.BaseEventHandler","title":"<code>BaseEventHandler(event_source, event_bus, provider)</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>ABC to handle event manipulation from a model, and publishing through a provider.</p> PARAMETER  DESCRIPTION <code>event_source</code> <p>Event source name, e.g., 'myorg.service.feature'</p> <p> TYPE: <code>str</code> </p> <code>event_bus</code> <p>Event bus to send events to</p> <p> TYPE: <code>str</code> </p> <code>provider</code> <p>Event Provider to publish events through.</p> <p> TYPE: <code>BaseEventProvider</code> </p> Source code in <code>product/stream_processor/integrations/events/base.py</code> <pre><code>@abstractmethod\ndef __init__(self, event_source: str, event_bus: str, provider: BaseEventProvider) -&gt; None:\n    \"\"\"ABC to handle event manipulation from a model, and publishing through a provider.\n\n    Parameters\n    ----------\n    event_source : str\n        Event source name, e.g., 'myorg.service.feature'\n    event_bus: str\n        Event bus to send events to\n    provider : BaseEventProvider\n        Event Provider to publish events through.\n    \"\"\"\n    self.provider = provider\n    self.event_source = event_source\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.BaseEventHandler-attributes","title":"Attributes","text":"<code>provider = provider</code> <code>instance-attribute</code> \u00b6 <code>event_source = event_source</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.base.BaseEventHandler-functions","title":"Functions","text":"<code>emit(payload, metadata=None, correlation_id='')</code> <code>abstractmethod</code> \u00b6 <p>Emits events using registered provider, along with additional metadata or specific correlation ID.</p> PARAMETER  DESCRIPTION <code>payload</code> <p>List of models to convert and publish as an Event. List of models to convert and publish as an Event.</p> <p> TYPE: <code>list[T]</code> </p> <code>metadata</code> <p>Additional metadata to be injected into the event before sending, by default None</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>correlation_id</code> <p>Correlation ID to inject in event metadata. We generate one if not provided.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>EventReceipt</code> <p>Receipts for unsuccessfully and successfully published events.</p> Source code in <code>product/stream_processor/integrations/events/base.py</code> <pre><code>@abstractmethod\ndef emit(self, payload: list[T], metadata: dict[str, Any] | None = None, correlation_id='') -&gt; EventReceipt:\n    \"\"\"Emits events using registered provider, along with additional metadata or specific correlation ID.\n\n    Parameters\n    ----------\n    payload : list[T]\n        List of models to convert and publish as an Event.\n        List of models to convert and publish as an Event.\n    metadata : dict[str, Any] | None, optional\n        Additional metadata to be injected into the event before sending, by default None\n    correlation_id : str, optional\n        Correlation ID to inject in event metadata. We generate one if not provided.\n\n    Returns\n    -------\n    EventReceipt\n        Receipts for unsuccessfully and successfully published events.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/stream_processor/#utility-functions","title":"Utility functions","text":"<p>Standalone functions related to events integration. These are reused in more than one location, and tested separately</p> <p>Constants related to events integration (event handler and event providers)</p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.functions-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.functions.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":"<p>Generic type for a list of events</p>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.functions-functions","title":"Functions","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.functions.chunk_from_list","title":"<code>chunk_from_list(events, max_items)</code>","text":"<p>Slices a list of items into a generator, respecting the max number of items.</p> PARAMETER  DESCRIPTION <code>events</code> <p>List of events to slice.</p> <p> TYPE: <code>list[T]</code> </p> <code>max_items</code> <p>Maximum number of items per chunk.</p> <p> TYPE: <code>int</code> </p> YIELDS DESCRIPTION <code>Generator[list[T], None, None]</code> <p>Generator containing batches of events with maximum number of items requested.</p> Source code in <code>product/stream_processor/integrations/events/functions.py</code> <pre><code>def chunk_from_list(events: list[T], max_items: int) -&gt; Generator[list[T], None, None]:\n    \"\"\"Slices a list of items into a generator, respecting the max number of items.\n\n    Parameters\n    ----------\n    events : list[T]\n        List of events to slice.\n    max_items : int\n        Maximum number of items per chunk.\n\n    Yields\n    ------\n    Generator[list[T], None, None]\n        Generator containing batches of events with maximum number of items requested.\n    \"\"\"\n    for idx in range(0, len(events), max_items):  # start, stop, step\n        # slice the first 10 items, then the next 10 items starting from the index\n        yield from [events[idx : idx + max_items]]\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.constants-attributes","title":"Attributes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.constants.DEFAULT_EVENT_VERSION","title":"<code>DEFAULT_EVENT_VERSION = 'v1'</code>  <code>module-attribute</code>","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.constants.EVENTBRIDGE_PROVIDER_MAX_EVENTS_ENTRY","title":"<code>EVENTBRIDGE_PROVIDER_MAX_EVENTS_ENTRY = 10</code>  <code>module-attribute</code>","text":""},{"location":"api/stream_processor/#exceptions","title":"Exceptions","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions-classes","title":"Classes","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions.NotificationDeliveryError","title":"<code>NotificationDeliveryError(message, receipts)</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a notification delivery fails.</p> PARAMETER  DESCRIPTION <code>message</code> <p>error message</p> <p> TYPE: <code>str</code> </p> <code>receipts</code> <p>list of receipts failed notification deliveries along with details</p> <p> TYPE: <code>list[EventReceiptFail]</code> </p> Source code in <code>product/stream_processor/integrations/events/exceptions.py</code> <pre><code>def __init__(self, message: str, receipts: list[EventReceiptFail]):\n    \"\"\"Exception raised when a notification delivery fails.\n\n    Parameters\n    ----------\n    message : str\n        error message\n    receipts : list[EventReceiptFail]\n        list of receipts failed notification deliveries along with details\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.receipts = receipts\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions.NotificationDeliveryError-attributes","title":"Attributes","text":"<code>message = message</code> <code>instance-attribute</code> \u00b6 <code>receipts = receipts</code> <code>instance-attribute</code> \u00b6"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions.NotificationDeliveryError-functions","title":"Functions","text":""},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions.ProductChangeNotificationDeliveryError","title":"<code>ProductChangeNotificationDeliveryError(message, receipts)</code>","text":"<p>             Bases: <code>NotificationDeliveryError</code></p> <p>Raised when one or all product change notification deliveries fail.</p> Source code in <code>product/stream_processor/integrations/events/exceptions.py</code> <pre><code>def __init__(self, message: str, receipts: list[EventReceiptFail]):\n    super().__init__(message, receipts)\n</code></pre>"},{"location":"api/stream_processor/#product.stream_processor.integrations.events.exceptions.ProductChangeNotificationDeliveryError-functions","title":"Functions","text":""}]}